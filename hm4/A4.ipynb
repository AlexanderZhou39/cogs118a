{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3745974",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"A4.ipynb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bbb4cd0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68f2f0ed9883b594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 4\n",
    "\n",
    "## **Due: Nov 8th (Friday), 2024, 11:59pm (Pacific Time)**\n",
    "\n",
    "### **Instructions:**\n",
    "\n",
    "Your Jupyter notebook assignment will often have 3 elements: written answers, code answers, and quiz answers. For written answers, you may insert images of your handwritten work in code cells, or write your answers in markdown and LaTeX. For quiz answers, your `record.txt` file will record your answer choices in the quiz modules for submission. Both your quiz answers and code answers will be autograded on Gradescope. This assignment does not have the quiz portion.\n",
    "\n",
    "For all elements, DO NOT MODIFY THE CELLS. Put your answers **only** in the answer cells given, and **do not delete cells**. If you fail to follow these instructions, you will lose points on your submission.\n",
    "\n",
    "Make sure to show the steps of your solution for every question to receive credit, not just the final answer. You may search information online but you will need to write code/find solutions to answer the questions yourself. You will submit your .ipynb file and record.txt to gradescope when you are finished.\n",
    "\n",
    "### **Late Policy:**\n",
    "\n",
    "5% reduction for the first day and 10% reduction afterwards for every extra day past due.\n",
    "\n",
    "### How to Include Your Math Written Answer?\n",
    "\n",
    "You could use markdowns' include image functionality (recommended) or $\\LaTeX$ in markdown to submit your written responses.\n",
    "\n",
    "#### Include Images (recommended)\n",
    "If you are still getting familiar with using LaTeX, handwrite the response on paper or the stylus. Take a picture or screenshot of your answer, and include that image in the Jupyter Notebook. Be sure to include that image in the `\\imgs` directory. Let's say you have your Q1 response saved as `imgs/Q1.png`; the markdown syntax to include that image is `![Q1](imgs/Q1.png)`.\n",
    "\n",
    "#### $\\LaTeX$\n",
    "[Here is a fantastic tutorial from CalTech about using $\\LaTeX$ in Jupyter Notebook.](http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html). You could also find various $\\LaTeX$ tutorials and cheat sheets online.\n",
    "\n",
    "## Important Notice\n",
    "\n",
    "You must check both submission output on the gradescope (`Assignment 4 - Notebook` and `Assignment 4 - Manual Grading`) correctly reflects your work and responses. If you notice inconsistencies between your notebook and the Manual Grading portion, you need to make a Piazza post, and we can help you with that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96a206c8",
   "metadata": {},
   "source": [
    "# Question 1: Conceptual Questions\n",
    "\n",
    "Select the correct option(s). Note that there might be multiple correct options.\n",
    "\n",
    "Write your solution as a list of strings by replacing the \"...\" \n",
    "\n",
    "Ex.: `[\"A\"]` if you think the answer is A, and `[\"A\", \"C\"]` if you think the answers are A and C). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf140878",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.1\n",
    "\n",
    "Choose **all** the valid answers to the description about **linear regression** and **logistic regression** from the options below:\n",
    "\n",
    "A. Linear regression is an unsupervised learning problem; logistic regression is a supervised learning problem. \n",
    "\n",
    "B. Linear regression deals with the prediction of continuous values; logistic regression deals with the prediction of class labels.\n",
    "\n",
    "C. We cannot use gradient descent to solve linear regression. Instead, we can only use the closed-form solution to tackle the linear regression problem.\n",
    "\n",
    "D. Linear regression is a convex optimization problem whereas logistic regression is not.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3cfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec326d0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q1_1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbcd68c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.2\n",
    "\n",
    "Choose **all** the valid answers to the description about **gradient descent** from the options below:\n",
    "\n",
    "\n",
    "A. The global minimum is guaranteed to be reached by using gradient descent.\n",
    "\n",
    "B. Every gradient descent iteration can always decrease the value of loss function even when the gradient of the loss function is zero.\n",
    "\n",
    "C. When the learning rate is very large, it is possible that some iterations of gradient descent may not decrease the value of loss function.\n",
    "\n",
    "D. With different initial weights, it is possible for the gradient descent algorithm to obtain to different local minimum.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c3781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8797c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q1_2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49d8676f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2 Error Metrics\n",
    "\n",
    "The grid below shows $5 \\times 4 = 20$ possible locations for targets to appear. We make guesses at the locations where targets are located, and mark those cells in blue. For the rest of the locations, we guess that those locations are non-targets, which are marked in white. The actual locations for the target are marked with a circle in cells, while the actual locations for non-target are marked with empty cells.\n",
    "\n",
    "<img src=\"imgs/error-metric.png\" width=\"20%\" />\n",
    "\n",
    "### 1. Please evaluate the following metrics for your guesses.\n",
    "\n",
    "- True Positive (variable name `TP`): The case which is target and predicted as target.\n",
    "- True Negative (variable name `TN`): The case which is non-target and predicted as non-target.\n",
    "- False Positive (variable name `FP`): The case which is non-target but predicted as target.\n",
    "- False Negative (variable name `FN`): The case which is target but predicted as non-target.\n",
    "\n",
    "Write your solution as a number by replacing the `...` part after each variable.\n",
    "\n",
    "Hint: In this question, all the numbers should be integers. Ex.: `TP=1`.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405bb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TP = ...\n",
    "TN = ...\n",
    "FP = ...\n",
    "FN = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ac392",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f497692",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2. Please evaluate the following metrics.\n",
    "\n",
    "You may use the results from the previous question.\n",
    "\n",
    "- Precision (variable name `precision`): \n",
    "$$\\text{Precision}=\\frac{\\text{number of true positives}}{\\text{number of guesses}}$$\n",
    "- Recall (variable name `recall`): \n",
    "$$\\text{Recall}=\\frac{\\text{number of true positives}}{\\text{number of actual targets}}$$\n",
    "- F-value (variable name `f_value`): \n",
    "$$\\text{F-value}=\\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "Write your solution as a number by replacing the `...` part after each variable. Please round your answers to **2 decimal places**.\n",
    "\n",
    "Hint: In this question, all the numbers should be floats. Ex.: `precision=0.60`.\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28949372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = ...\n",
    "recall = ...\n",
    "f_value = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5800f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q2_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14dffe",
   "metadata": {},
   "source": [
    "# Question 3: Linear Regression\n",
    "\n",
    "Assume we are given a dataset $S = {(x_i, y_i) | i \\in {1, . . . , n}}$. Here, $x_i \\in \\mathbb{R}$ is a feature scalar (a.k.a. value of input variable) and $y_i \\in \\mathbb{R}$ is its corresponding value (a.k.a. value of dependent variable). In this section, we aim to fit data points with a line:\n",
    "\n",
    "$$ y = w_0 + w_1x $$\n",
    "\n",
    "where $w_0$, $w_1 \\in \\mathbb{R}$ are two parameters to determine the line. Next, we measure the quality of fitting by evaluating a sum-of-squares error function $g(w_0, w_1)$ :\n",
    "\n",
    "$$ g(w_0, w_1) = \\sum_{i=1}^{n} (w_0 + w_1 x_i - y_i)^2 $$\n",
    "\n",
    "When $g(w_0, w_1)$ is near zero, it means the proposed line can fit the dataset and model and model an accurate relation between $x_i$ and $y_i$. The best linewith parameters $(w^∗_0, w^∗_1)$ can reach the minimum value of the error function $g(w_0, w_1)$:\n",
    "\n",
    "$$(w^∗_0, w^∗_1) = \\underset{w_0, w_1}{\\operatorname{argmin}} \\, g(w_0, w_1)$$\n",
    "\n",
    "To obtain the parameters of the best line, we will take the gradient of function $g(w_0, w_1)$ and set it to zero. That is:\n",
    "\n",
    "$$\\nabla g(w_0, w_1) = 0$$\n",
    "\n",
    "The solution $(w^*_0, w^*_1)$ of the above equation will determine the best line $y=w^∗_0 + w^∗_1 x$ that fits the dataset $S$.\n",
    "In reality, we typically tackle this task in a matrix form: First, we represent data points as matrices $X = [x_1, x_2, . . . , x_n]^T$ and $Y = [y_1, y_2, . . . , y_n]^\\top$, where $x_i = [1, x_i]^\\top$ is a feature vector corresponding to $x_i$. The parameters of the line are also represented as a matrix $W = [w_0, w_1]^T$. Thus, the sum-of-squares error\n",
    "function $g(W)$ can be defined as (a.k.a. squared $L_2$ norm):\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g(W) &= \\sum_{i=1}^{n}(x_i^TW - y_i)^2 \\\\\n",
    " &= || XW − Y ||_2^2 \\\\\n",
    " &= (XW − Y )^\\top(XW-Y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Similarly, the parameters $W^∗ = [w^∗_0, w^∗_1]^\\top$ of the best line can be obtained by solving the equation below:\n",
    "\n",
    "$$ \\nabla g(W) = \\frac{\\partial g(W)}{\\partial W} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dd752",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3.1 OLS Matrix Form\n",
    "According to the definition of $g(W)$ above, compute the gradient of $g(W)$ with respect to $W$. Your result should be in the form of $X$, $Y$, and $W$. \n",
    "\n",
    "Show your work.\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d958e",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75b290",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3.2: OLS Matrix Solution\n",
    "\n",
    "By setting the answer of last part to 0, prove the following:\n",
    "\n",
    "$$ W^∗ = \\underset{W}{\\operatorname{argmin}} \\, g(W) = (X^TX)^{-1}X^TY$$\n",
    "\n",
    "Note: The above formula demonstrates a closed form solution of $$\\nabla g(W) = \\frac{\\partial g(W)}{\\partial W} = 0$$\n",
    "\n",
    "Show your work.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de85d74",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ac309",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## (Extra Credit) Question 3.3: L-1 Error\n",
    "\n",
    "Previously, we define a sum-of-squares error function $g(w_0, w_1) = \\sum_{i=1}^{n} (y_i - w_0 - w_1 x_1)^2$ and represent it in a matrix form $g(W) = \\left\\lVert X W - Y \\right\\rVert_2^2$. Actually, we can have multiple choices of the error function: For example, we can define a sum-of-absolute error function $h(w_0, w_1)$:\n",
    "$$\n",
    "    h(w_0, w_1) = \\sum_{i=1}^{n} |w_0 + w_1 x_i - y_i|\n",
    "$$\n",
    "and represent it in a matrix form $h(W)$ (a.k.a. $L_1$ norm):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    h(W) & = \\sum_{i=1}^{n} |\\mathbf{x}_i^T W - y_i| \\\\\n",
    "         & = \\left\\lVert X W - Y \\right\\rVert_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "According to the information above, compute the gradient of the error function $h(W)$ with respect to $W$. Your result should be in the form of $\\mathbf{x}_i$, $y_i$ and $W$.\n",
    "\n",
    "**Hint**: Given a function $f(\\mathbf{x}) \\in \\mathbb{R}$, we have:\n",
    "$$ \\frac{\\partial |f(\\mathbf{x})|}{\\partial \\mathbf{x}} = \\text{sign}(f(\\mathbf{x}))\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}} $$\n",
    "\n",
    " where\n",
    "$$\n",
    "\\text{sign}(x) = \\left\\{\n",
    "             \\begin{array}{cl}\n",
    "              1,  & x > 0 \\\\\n",
    "              0,  & x = 0 \\\\\n",
    "             -1,  & x < 0.\n",
    "             \\end{array}  \n",
    "        \\right. \\nonumber\n",
    "$$\n",
    "\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6ceb5",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9de6f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## (You DO NOT need to solve this problem) Question 3.4 \n",
    "\n",
    "\n",
    "In fact, the gradient in Q3.3 can also be represented as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla h(W) = \\frac{\\partial h(W)}{\\partial W} = \\Big(\\big(\\text{sign}(X W-Y)\\big)^{\\top} X\\Big)^\\top\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\text{sign}(A)$ means performing element-wise $\\text{sign}(a_{ij})$ over all element $a_{ij}$ in a matrix $A$. This matrix form of gradient can be helpful in some following coding questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabeac1c",
   "metadata": {},
   "source": [
    "# Question 4: Parabola Estimation\n",
    "\n",
    "We are given a dataset $S=\\{(x_i,y_i), i=1,\\ldots,n\\}$. Here, $x_i$ is a feature scalar. In this section, we aim to fit data points with a parabola:\n",
    "$$\n",
    "    y = w_0 + w_1 x + w_2 x^2 \n",
    "$$\n",
    "where $w_0, w_1, w_2 \\in \\mathbb{R}$ are three parameters to determine the parabola. Then, we represent the data points as matrices $X=[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n]^\\top$ and $Y=[y_1, y_2, \\ldots, y_n]^\\top$, where $\\mathbf{x}_i=[1, x_i, x_i^2]^\\top$ is a feature vector corresponding to the data $x_i$. The parameters of the parabola are also represented as a matrix $W=[w_0, w_1, w_2]^\\top$. Next, we define some loss function $\\mathcal{L}(W)$ and attempt to obtain the best parameters $W^*$ that minimizes $\\mathcal{L}(W)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Important packages (nothing to add to this cell)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54499fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (nothing to add to this cell)\n",
    "\n",
    "X_and_Y = np.load('./parabola_estimation.npy')\n",
    "old_X = X_and_Y[:, 0]  # Shape: (300,)\n",
    "Y = X_and_Y[:, 1]  # Shape: (300,)\n",
    "old_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53828d",
   "metadata": {},
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca351df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the original datapoints (nothing to add to this cell).\n",
    "\n",
    "def vis(w0, w1, w2):\n",
    "    draw_plane = (w0 is not None) and (w1 is not None) and (w2 is not None)\n",
    "    if draw_plane:\n",
    "        X_line = np.linspace(0,10,300)\n",
    "        Y_line = w0 + w1 * X_line + w2 * (X_line**2)\n",
    "        plt.plot(X_line, Y_line, color='orange')\n",
    "        \n",
    "    plt.scatter(old_X, Y, color='gray')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    \n",
    "vis(None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3d2b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4.1: Parabola Estimation with Squared $L_2$ Norm\n",
    "Consider squared $L_2$ norm as the loss function $\\mathcal{L}(W)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathcal{L}(W) & = \\sum_{i=1}^{n} (\\mathbf{x}_i^T W - y_i)^2  \\\\\n",
    "    & = \\left\\lVert X  W - Y \\right\\rVert_2^2 \\\\\n",
    "    & = (X W - Y)^T (X  W - Y).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Complete the following code to use the **closed form solution** to compute $W^* = \\arg\\min_{W} \\mathcal{L}(W)$ and plot the scatter graph of data and estimated parabola.\n",
    "\n",
    "Write your code in the `...` part.\n",
    "\n",
    "**Hint**: You may refer to Q3.2 for the analytic solution. Also, `np.hstack` may be useful for constructing the design matrix $X$.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf16cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def l2_analytic_estimation(old_X, Y):\n",
    "    # Construct a design matrix of shape (300,3)\n",
    "    X = ...\n",
    "\n",
    "    # Hint: In the form of X and Y and should have a shape of (3,1)\n",
    "    W = ...\n",
    "\n",
    "    w0, w1, w2 = ...\n",
    "    return X, np.array([w0, w1, w2])\n",
    "\n",
    "X, W = l2_analytic_estimation(old_X, Y)\n",
    "w0, w1, w2 = W\n",
    "print('y = {:.2f} + {:.2f}*x + {:.2f}*x^2'.format(w0, w1, w2))\n",
    "vis(w0, w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd67ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4.2 Parabola Estimation with $L_1$  Norm\n",
    "\n",
    "Consider $L_1$ norm as the loss function $\\mathcal{L}(W)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(W) & = \\sum_{i=1}^{n} |\\mathbf{x}_i^T W - y_i| \\\\\n",
    "& = \\left\\lVert X W - Y \\right\\rVert_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this problem, we would like to use the **gradient descent** to calculate the parameters $W$ for the parabola.\n",
    "\n",
    "If we have a loss function $\\mathcal{L}(W)$, then a typical gradient descent algorithm contains the following steps:\n",
    "\n",
    "**Step 1**. Initialize the parameters W.\n",
    "\n",
    "for i = 1 to `iterations`:\n",
    "\n",
    "- **Step 2**. Compute the gradient $\\nabla \\mathcal{L}(W) = \\frac{\\partial \\mathcal{L}(W)}{\\partial W}$.\n",
    "\n",
    "- **Step 3**. Update the parameters $W \\leftarrow \\mathcal{L}(W) = W - \\eta \\frac{\\partial \\mathcal{L}(W)}{\\partial W}$ where $\\eta$ is the learning rate.\n",
    "\n",
    "Please complete the following code to calculate the gradient for the loss function, and perform the gradient descent algorithm. \n",
    "\n",
    "Write your code in the `...` part.\n",
    "\n",
    "**Hint 1**: You may refer to Q3.4 for the gradient of $L_1$ norm. \n",
    "\n",
    "**Hint 2**: You may use your previous design matrix of $X$.\n",
    "\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67324210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W) with respect to W \n",
    "#(you need to add code to this cell as indicated below).\n",
    "\n",
    "def grad_L_W_ver1(X, Y, W):\n",
    "    grad = ...\n",
    "    assert grad.shape == (3,1)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5626b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimating W, which defines the hyperplane using gradient descent (you need to add code to this cell as indicated below).\n",
    "# y = w0 + w1*x + w2*x^2\n",
    "def l1_grad_descent(X, Y):\n",
    "    assert X.shape == (300,3), f\"Your design matrix is shaped wrong, got {X.shape}\"\n",
    "    \n",
    "    # Some settings.\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    iterations    = 300000\n",
    "    learning_rate = 0.000001\n",
    "\n",
    "    # Gradient descent algorithm.\n",
    "    # Step 1. Initialize the parameters W using np.zeros\n",
    "    W = ...\n",
    "\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Step 2. Calculate the gradient of L(W) w.r.t. W. \n",
    "        grad = ...\n",
    "        # Step 3. Update parameters W.\n",
    "        ...\n",
    "\n",
    "    # Store the parameters of the parabola.\n",
    "    w0, w1, w2 = np.array(W).reshape(-1)\n",
    "    \n",
    "    return np.array([w0,w1,w2])\n",
    "\n",
    "# Visualization.\n",
    "w0, w1, w2 = l1_grad_descent(X, Y)\n",
    "print('y = {:.2f} + {:.2f}*x + {:.2f}*x^2'.format(w0, w1, w2))\n",
    "vis(w0, w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7c8d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4.3 Parabola Estimation with Squared $L_2$ Norm and $L_1$ Norm\n",
    "\n",
    "In this problem, we would like to use the gradient descent to calculate the parameters $W$ for the parabola.\n",
    "The loss function $\\mathcal{L}(W)$ now contains two parts: A squared $L_2$ norm and a $L_1$ norm.\n",
    "A coefficient $\\alpha$ is used to control the ratio of these two norms:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(W) \n",
    "& = \\sum_{i=1}^{n} \n",
    "\\Big(\\alpha\\big(\\mathbf{x}_i^T W - y_i\\big)^2 + (1-\\alpha)|\\mathbf{x}_i^T W - y_i| \\Big) \\\\\n",
    "& = \\alpha\\left\\lVert X W - Y \\right\\rVert_2^2 + (1-\\alpha)\\left\\lVert X W - Y \\right\\rVert_1 \\\\\n",
    "\\nonumber\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Complete the following code to use the **gradient descent** to find $W^*$ when $\\alpha=0, \\alpha=0.03, \\alpha=0.05, \\alpha=0.1$, and $\\alpha=1$, espectively.\n",
    "\n",
    "Write your code in the `...` part.\n",
    "\n",
    "**Hint**: You may refer to Q3.1 for the gradient of $L_2$ norm. \n",
    "\n",
    "**Note:** It may take 2~3 mins to run the algorithm.\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118da00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W) with respect to W (you need to add code to this cell as indicated below).\n",
    "def grad_L_W_ver2(X, Y, W, alpha):\n",
    "    grad = ...\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d7dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to use gradient descent to estimate parabola given a list of alphas\n",
    "# (you need to add code to this cell as indicated below).\n",
    "# Hint: For each alpha, you need to use gradient descent, hence, you need to write a loop inside the loop.\n",
    "\n",
    "def l1_l2_grad_descent(old_X, X, Y, alpha_list):\n",
    "    # Some settings\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    iterations    = 300000\n",
    "    learning_rate = 0.000001\n",
    "    \n",
    "    w_history = []\n",
    "    # Loop over alpha(s).\n",
    "    ...\n",
    "        \n",
    "        # Gradient descent algorithm.\n",
    "        # Step 1. Initialize the parameters W.\n",
    "        W = ...\n",
    "        for i in range(iterations):\n",
    "            # Step 2. Calculate the gradient of L(W) w.r.t. W.\n",
    "            grad = ...\n",
    "            # Step 3. Update parameters W.\n",
    "            W = ...\n",
    "\n",
    "        # Get the parameters of the parabola.\n",
    "        w0, w1, w2 = np.array(W).reshape(-1)\n",
    "        w_history.append((w0,w1,w2))\n",
    "        \n",
    "    return np.array(w_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e36d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate the parabolas given the list of alpha(s).\n",
    "alphas = [0, 0.03, 0.05, 0.1, 1]\n",
    "w_history = l1_l2_grad_descent(old_X, X, Y, alpha_list=alphas)\n",
    "\n",
    "# print and plot the result\n",
    "plt.scatter(old_X, Y, color='gray')\n",
    "for alpha, ws in zip(alphas, w_history):\n",
    "    w0, w1, w2 = ws\n",
    "    \n",
    "    # plot\n",
    "    X_line = np.linspace(0,10,300)\n",
    "    Y_line = w0 + w1 * X_line + w2 * (X_line**2)\n",
    "    plt.plot(X_line, Y_line, label='alpha={}'.format(alpha))\n",
    "    \n",
    "    # print\n",
    "    print('When alpha = {},'.format(alpha))\n",
    "    print('y = {:.2f} + {:.2f}*x + {:.2f}*x^2'.format(w0, w1, w2))\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67249b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 4.4 Comparison\n",
    "\n",
    "\n",
    "Compare the parabolas in Q5.3. Try to explain the trend from $\\alpha = 0$ (i.e. $L_1$\n",
    " norm), $\\alpha = 0.03$, $\\alpha = 0.05$, $\\alpha = 0.1$, till $\\alpha = 1$ (i.e. squared $L_2$ norm).\n",
    "\n",
    "You may type your answers directly in the Markdown cell below.\n",
    "\n",
    "\n",
    "**Hint**: You may need to consider the outliers in the data points for your reasoning.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf0d67",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5630b26b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# End of A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538e610",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please make sure to see the output of the gradescope autograder. You are responsible for waiting and ensuring that the autograder is executing normally for your submission. Please create a Piazza post if you see errors in autograder execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8248cff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True, run_tests=True, files=['imgs', 'requirements.txt', 'parabola_estimation.npy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481f0d2",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "cogs118a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": false,
   "tests": {
    "Q1_1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1_1\"\npoints = 2\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q1_1_sanity_check(q1_1):\n    all_options = [\"A\", \"B\", \"C\", \"D\"]\n    check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n    assert check_valid(q1_1, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
    "Q1_2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1_2\"\npoints = 2\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q1_2_sanity_check(q1_2):\n    all_options = [\"A\", \"B\", \"C\", \"D\"]\n    check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n    assert check_valid(q1_2, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
    "Q2_1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q2_1\"\npoints = 4\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q2_1_sanity_check(env):\n    def has_variable(var):\n        assert var in env, \"Did you define the variable {}?\".format(var)\n    has_variable(\"TP\")\n    has_variable(\"TN\")\n    has_variable(\"FP\")\n    has_variable(\"FN\")\n\n    def correct_type(var, expected_type):\n        assert isinstance(env[var], expected_type), \"Did you define {} to be of type {}?\".format(var, expected_type)\n\n    correct_type(\"TP\", int)\n    correct_type(\"TN\", int)\n    correct_type(\"FP\", int)\n    correct_type(\"FN\", int)\n",
    "Q2_2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q2_2\"\npoints = 6\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q2_2_sanity_check(env):\n    def has_variable(var):\n        assert var in env, \"Did you define the variable {}?\".format(var)\n    has_variable(\"precision\")\n    has_variable(\"recall\")\n    has_variable(\"f_value\")\n\n    def correct_type(var, expected_type):\n        assert isinstance(env[var], expected_type), \"Did you define {} to be of type {}?\".format(var, expected_type)\n\n    correct_type(\"precision\", float)\n    correct_type(\"recall\", float)\n    correct_type(\"f_value\", float)\n@test_case(points=2, hidden=False)\ndef q2_2_precision_check(precision):\n    assert abs(round(precision, 2) - round(1 / 2, 2)) == 0\n@test_case(points=1, hidden=False)\ndef q2_2_recall_check(recall):\n    assert abs(round(recall, 2) - round(2 / 3, 2)) <= 0.01\n@test_case(points=1, hidden=False)\ndef q2_2_recall_check_strict(recall):\n    assert abs(round(recall, 2) - round(2 / 3, 2)) == 0\n@test_case(points=1, hidden=False)\ndef q2_2_f_value_check(f_value):\n    assert abs(round(f_value, 2) - round(4 / 7, 2)) <= 0.01\n@test_case(points=1, hidden=False)\ndef q2_2_f_value_check_strict(f_value):\n    assert abs(round(f_value, 2) - round(4 / 7, 2)) == 0\n",
    "Q4_parabola_l1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q4_parabola_l1\"\npoints = 4\n\n",
    "Q4_parabola_l1_l2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q4_parabola_l1_l2\"\npoints = 6\n\n",
    "Q4_parabola_l2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q4_parabola_l2\"\npoints = 3\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
