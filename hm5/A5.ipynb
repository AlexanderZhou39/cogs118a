{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054ffe5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"A5.ipynb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bbb4cd0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68f2f0ed9883b594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 5\n",
    "\n",
    "## **Due: Nov 17th (Sunday), 2024, 11:59pm (Pacific Time)**\n",
    "\n",
    "### **Instructions:**\n",
    "\n",
    "Your Jupyter notebook assignment will often have 3 elements: written answers, code answers, and quiz answers. For written answers, you may insert images of your handwritten work in code cells, or write your answers in markdown and LaTeX. For quiz answers, your `record.txt` file will record your answer choices in the quiz modules for submission. Both your quiz answers and code answers will be autograded on Gradescope. This assignment does not have the quiz portion.\n",
    "\n",
    "For all elements, DO NOT MODIFY THE CELLS. Put your answers **only** in the answer cells given, and **do not delete cells**. If you fail to follow these instructions, you will lose points on your submission.\n",
    "\n",
    "Make sure to show the steps of your solution for every question to receive credit, not just the final answer. You may search information online but you will need to write code/find solutions to answer the questions yourself. You will submit your .ipynb file and record.txt to gradescope when you are finished.\n",
    "\n",
    "### **Late Policy:**\n",
    "\n",
    "5% reduction for the first day and 10% reduction afterwards for every extra day past due.\n",
    "\n",
    "### How to Include Your Math Written Answer?\n",
    "\n",
    "You could use markdowns' include image functionality (recommended) or $\\LaTeX$ in markdown to submit your written responses.\n",
    "\n",
    "#### Include Images (recommended)\n",
    "If you are still getting familiar with using LaTeX, handwrite the response on paper or the stylus. Take a picture or screenshot of your answer, and include that image in the Jupyter Notebook. Be sure to include that image in the `\\imgs` directory. Let's say you have your Q1 response saved as `imgs/Q1.png`; the markdown syntax to include that image is `![Q1](imgs/Q1.png)`.\n",
    "\n",
    "#### $\\LaTeX$\n",
    "[Here is a fantastic tutorial from CalTech about using $\\LaTeX$ in Jupyter Notebook.](http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html). You could also find various $\\LaTeX$ tutorials and cheat sheets online.\n",
    "\n",
    "## Important Notice\n",
    "\n",
    "You must check both submission output on the gradescope (`Assignment 5 - Notebook` and `Assignment 5 - Manual Grading`) correctly reflects your work and responses. If you notice inconsistencies between your notebook and the Manual Grading portion, you need to make a Piazza post, and we can help you with that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96a206c8",
   "metadata": {},
   "source": [
    "# Question 1: Conceptual Questions\n",
    "\n",
    "Select the correct option(s). Note that there might be multiple correct options.\n",
    "\n",
    "Write your solution as a list of strings by replacing the \"...\" \n",
    "\n",
    "Ex.: `[\"A\"]` if you think the answer is A, and `[\"A\", \"C\"]` if you think the answers are A and C). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf140878",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.1 Logistic Regression\n",
    "\n",
    "Choose **all** the valid answers to the description about **linear regression** and **logistic regression** from the options below:\n",
    "\n",
    "A. Linear regression is an unsupervised learning problem; logistic regression is a supervised learning problem. \n",
    "\n",
    "B. Linear regression deals with the prediction of continuous values; logistic regression deals with the prediction of class labels.\n",
    "\n",
    "C. We cannot use gradient descent to solve linear regression. Instead, we can only use the closed-form solution to tackle the linear regression problem.\n",
    "\n",
    "D. Linear regression is a convex optimization problem whereas logistic regression is not.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3cfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e2562",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q1_1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbcd68c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.2 Support Vector Machine (SVM)\n",
    "\n",
    "In the Figure 1, we use a black line to mark the decision boundary of a linear SVM classifier parameterized by weight vector $\\mathbf{w}$ and bias $b$. If we start to increase the margin of the classifier, what would happen to $\\mathbf{w}$? **(SVM is expected to be covered on Nov 16th.)**\n",
    "\n",
    "<div align=\"center\"> <img src=\"imgs/svm.png\" width=\"50%\"/><br><em>A linear SVM classifier with data points</em></div>\n",
    "\n",
    "\n",
    "A. The magnitude of $\\mathbf{w}$ does not change.\n",
    "\n",
    "B. $\\mathbf{w}$ will have smaller magnitude.\n",
    "\n",
    "C. $\\mathbf{w}$ will have greater magnitude.\n",
    "\n",
    "D. None of above.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c3781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c14dd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q1_2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c693d4bc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.3 Losgistic Regression\n",
    "\n",
    "\n",
    "Assume we have a training dataset $S=\\{(\\mathbf{x}_i, y_i), i=1, \\dots, n\\}$, and we want to train a logistic regression classifier on this training set.\n",
    "\n",
    "Question: Which is the correct answer for the optimal solution of model parameters $\\mathbf{w}$ and $b$?\n",
    "\n",
    "A. $(\\mathbf{w}^*, b^*) = \\arg \\min_{(\\mathbf{w}, b)} \\sum_{i=1}^n -\\ln(\\frac{1}{1+e^{-y_i(\\mathbf{w}^\\top\\mathbf{x}_i+b)}})$\n",
    "\n",
    "B. $(\\mathbf{w}^*, b^*) = \\arg \\max_{(\\mathbf{w}, b)} \\sum_{i=1}^n -\\ln(\\frac{1}{1+e^{-y_i(\\mathbf{w}^\\top\\mathbf{x}_i+b)}})$\n",
    "\n",
    "C. $(\\mathbf{w}^*, b^*) = \\arg \\min_{(\\mathbf{w}, b)} -\\ln(\\sum_{i=1}^n \\frac{1}{1+e^{-y_i(\\mathbf{w}^\\top\\mathbf{x}_i+b)}})$\n",
    "\n",
    "D. $(\\mathbf{w}^*, b^*) = \\arg \\max_{(\\mathbf{w}, b)} -\\sum_{i=1}^n (\\frac{1}{1+e^{-y_i(\\mathbf{w}^\\top\\mathbf{x}_i+b)}})$\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37984452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_3 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cb8c3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q1_3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49d8676f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2 Logistic Regression: Formulation\n",
    "\n",
    "Assume in a binary classification problem, we need to predict a binary label $y \\in \\{−1, +1\\}$ for a feature vector $x = [x_0, x_1]^\\top$. In logistic regression, we can reformulate the binary classification problem in a probabilistic framework: We aim to model the distribution of classes given the input feature vector x. Specifically, we can express the conditional probability p(y|x) parameterized by (w, b) using a logistic function. Assume the probability of the positive prediction $p(y = +1|\\mathbf{x})$ is represented as:\n",
    "\n",
    "$$\n",
    "p(y=+1|\\mathbf{x}) = \\frac{1}{1+e^{-(\\mathbf{w}^\\top x+b)}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42abb473",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.1 \n",
    "Please derive the formulation of $p(y=-1|\\mathbf{x})$.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1f3e6",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beb7cc9d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2\n",
    "Please show that $p(y|\\mathbf{x}) = \\frac{1}{1+e^{-y(\\mathbf{w}^\\top\\mathbf{x} + b)}}$.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28934db8",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e14dffe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Question 3: Logistic Regression Inference\n",
    "\n",
    "We have a logistic regression classifier for a 2-dimensional feature vector $\\mathbf{x} = (x_1, x_2)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{x}) = \\begin{cases}\n",
    "1, & \\text{if } \\frac{1}{1+e^{-(2x_1-x_2+1)}}\\geq 0.5, \\\\\n",
    "-1, & \\text{otherwise}. \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $f(\\mathbf{x}) \\in \\{-1, 1\\}$ is the predicted label. Please solve the following problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08acb2f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3.1\n",
    "Draw the decision boundary of the logistic regression classifier and shade the region where the classifier predicts 1.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6074931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x1_values = np.linspace(-2, 2, 400)\n",
    "x2_values = ...\n",
    "\n",
    "# Plot the line\n",
    "# Hint: You may use: plt.plot\n",
    "...\n",
    "\n",
    "# Fill the area below the line\n",
    "# Hint: You may use: plt.fill_between\n",
    "...\n",
    "\n",
    "# Axis labels and limits\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.ylim(-3.5, 5.5)\n",
    "\n",
    "# Draw the figure\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "118dd752",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 3.2\n",
    "There are two new data points $x_1 = (3, 10)$ and $x_2 = (10, 3)$. Predict the corresponding label $y_1, y_2 \\in \\{-1, 1\\}$ for $x_1$ and $x_2$ using the given logistic regression classifier.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1d324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y1 = ...\n",
    "y2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601da296",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q3_2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6eb9729",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4 \n",
    "In this section we want to calculate loss values for a perception model with parameters $\\mathbf{w}$ and $b$. The loss function is defined as:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}_i, y_i) = \\max(0, -y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b))\n",
    "$$\n",
    "\n",
    "and the gradient of the loss with respect to the parameter $\\mathbf{w}$ for the data point $\\mathbf{x}_i$ is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}} = \\begin{cases}\n",
    "0, & \\text{if } y_i=\\text{sign}(\\mathbf{w}^\\top\\mathbf{x}_i + b), \\\\\n",
    "-y_i\\mathbf{x}_i, & \\text{otherwise}. \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "We have a perception model for a 2-dimensional feature vector $\\mathbf{x} = (x_1, x_2)$:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{sign}(x_1 - 2x_2 + 1)\n",
    "$$\n",
    "\n",
    "The output of $f$ is the predicted label. $f(\\mathbf{x}) \\in \\{-1, 1\\}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0bba3c7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4.1 Loss Calculation\n",
    "Given four data points and their labels:\n",
    "* $\\mathbf{x}_1 = (-1, -1)$; $y_1=-1$\n",
    "* $\\mathbf{x}_2 = (1, 0)$; $y_2=1$\n",
    "* $\\mathbf{x}_3 = (2, 3)$; $y_3=1$\n",
    "* $\\mathbf{x}_4 = (3, -2)$; $y_4=1$\n",
    "\n",
    "Please calculate the loss value $\\mathcal{L}(\\mathbf{x}_i, y_i)$ for each data point.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee97984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# L_i: loss value for the i-th data point\n",
    "L_1 = ...\n",
    "L_2 = ...\n",
    "L_3 = ...\n",
    "L_4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23b774",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q4_1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b19d762",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4.2 Gradient Calculation\n",
    "Please calculate $\\frac{\\partial \\mathcal{L}(\\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}}$, the gradient of loss function with respect to $\\mathbf{w}$ for these four data points $\\{(\\mathbf{x}_i, y_i), i=1,2,3,4\\}$.\n",
    "\n",
    "Hint: The gradient could be a vector or 0. For consistency in your answers, represent the gradient as a list of length 2, such as [3, 4]. **If the gradient is 0, please use [0, 0] instead of a single 0**.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925fc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dL_dw_i: derivative of loss value with respect to w for the i-th data point\n",
    "dL_dw_1 = ...\n",
    "dL_dw_2 = ...\n",
    "dL_dw_3 = ...\n",
    "dL_dw_4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57270c02",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q4_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7857e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Question 5: Perceptron\n",
    "\n",
    "In this section, we will apply perceptron learning algorithm to solve the binary classification problem: We need to predict a binary label $y\\in \\{-1,1\\}$ for a feature vector $\\mathbf{x}=[x_0,x_1]^\\top$. The decision rule of the perceptron model is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{x}; \\mathbf{w}, b)=\\begin{cases}\n",
    "1, & \\text{if }\\mathbf{w}^\\top\\mathbf{x}+b\\geq 0, \\\\\n",
    "-1, & \\text{otherwise}. \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}=[w_0,w_1]^\\top$ is the weight vector, and $b$ is the bias scalar. Given a training dataset $S_\\text{training} = \\{(\\mathbf{x}_i, y_i)\\}, i=1,\\ldots,n\\}$, we define the training error $e_\\text{training}$ as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "e_\\text{training} = \\frac{1}{n}\\sum_{i=1}^n\\mathbf{1}\\big(y_i \\neq f(\\mathbf{x}_i;\\mathbf{w}, b)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and the test error $e_\\text{test}$ on the test set $S_\\text{test}$ can be defined in the same way. \n",
    "\n",
    "\n",
    "In the perceptron algorithm, we aim to directly minimize the training error $e_\\text{training}$ in order to obtain the optimal parameters $\\mathbf{w}^*, b^*$. If we represent data points in training set $S_\\text{training}$ as matrices $X=[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n]^T$ and $Y=[y_1, y_2, \\ldots, y_n]^T$, the perceptron algorithm is shown as below:\n",
    "\n",
    "<img src=\"imgs/perceptron.png\" alt=\"perceptron_algorithm\" width=\"80%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373993d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1f1c75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8b1ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    # Iris dataset.\n",
    "    iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "    X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                    # there are 150 data points, each data point\n",
    "                                    # has 4 features.\n",
    "\n",
    "    # Here for convenience, we divide the 3 kinds of flowers into 2 groups: \n",
    "    #     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "    #     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "    # Thus we use (iris.target > 1.5) to divide the targets into 2 groups. \n",
    "    # This line of code will assign:\n",
    "    #    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "    #    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "    Y = (iris.target > 1.5).reshape(-1,1).astype(float) # The shape of Y is (150, 1), which means \n",
    "                                    # there are 150 data points, each data point\n",
    "                                    # has 1 target value. \n",
    "    Y[Y==0] = -1\n",
    "\n",
    "    X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "    np.random.seed(1)               # Set the random seed.\n",
    "    np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "    print(\"X.shape\", X.shape)\n",
    "    print(\"Y.shape\", Y.shape)\n",
    "    print(\"X_and_Y[0]\", X_and_Y[0])  # The result should be always: [ 5.8  4.   1.2  0.2  0. ]\n",
    "\n",
    "    # Divide the data points into training set and test set.\n",
    "    X_shuffled = X_and_Y[:,:4]\n",
    "    Y_shuffled = X_and_Y[:,4]\n",
    "\n",
    "\n",
    "    X_train = X_shuffled[:100][:,[3,1]] # Shape: (100,2)\n",
    "    X_train = np.delete(X_train, 42, axis=0) # Remove a point for separability.\n",
    "    Y_train = Y_shuffled[:100]          # Shape: (100,)\n",
    "    Y_train = np.delete(Y_train, 42, axis=0) # Remove a point for separability.\n",
    "    X_test = X_shuffled[100:][:,[3,1]]  # Shape: (50,2)\n",
    "    Y_test = Y_shuffled[100:]           # Shape: (50,)\n",
    "    print(\"X_train.shape\", X_train.shape)\n",
    "    print(\"Y_train.shape\", Y_train.shape)\n",
    "    print(\"X_test.shape\", X_test.shape)\n",
    "    print(\"Y_test.shape\", Y_test.shape)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = construct_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb04bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a104d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def vis(X, Y, W=None, b=None):\n",
    "    indices_neg1 = (Y == -1).nonzero()[0]\n",
    "    indices_pos1 = (Y == 1).nonzero()[0]\n",
    "    plt.scatter(X[:,0][indices_neg1], X[:,1][indices_neg1], \n",
    "                c='blue', label='class -1')\n",
    "    plt.scatter(X[:,0][indices_pos1], X[:,1][indices_pos1], \n",
    "                c='red', label='class 1')\n",
    "    plt.legend()\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "    \n",
    "    if W is not None:\n",
    "        # w0x0+w1x1+b=0 => x1=-w0x0/w1-b/w1\n",
    "        w0 = W[0]\n",
    "        w1 = W[1]\n",
    "        temp = -w1*np.array([X[:,1].min(), X[:,1].max()])/w0-b/w0\n",
    "        x0_min = max(temp.min(), X[:,0].min())\n",
    "        x0_max = min(temp.max(), X[:,1].max())\n",
    "        x0 = np.linspace(x0_min,x0_max,100)\n",
    "        x1 = -w0*x0/w1-b/w1\n",
    "        plt.plot(x0,x1,color='black')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2806deb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Visualize training set.\n",
    "vis(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c28c3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Visualize test set.\n",
    "vis(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d487a21",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 5.1\n",
    "\n",
    "In this problem, we would like to train a perceptron model for the classification task on a modified Iris dataset. Please complete the following code (you should only insert your code in the `...` part) to implement the algorithm above.\n",
    "\n",
    "Note that in the code, we use `X_train` and `Y_train` to represent the feature vector $X$ and labels $Y$ in training set $S_\\text{training}$. Besides, we use `W` and `b` to represent the weight vector $\\mathbf{w}$ and bias scalar $b$.\n",
    "\n",
    "**Hint**: For the implementation of some functions, you may refer to HW3 Q4.\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0ab34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Judge function: 1(a != b).\n",
    "def judge(a, b):\n",
    "    \"\"\"\n",
    "    Judge function: 1(a != b).\n",
    "    Return 1 if a != b, otherwise return 0.\n",
    "    \"\"\"\n",
    "    if a != b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def f_perceptron(x, W, b):\n",
    "    \"\"\"\n",
    "    Perceptron classifier: f(x, W, b)\n",
    "    This function should return -1 or 1.\n",
    "\n",
    "    x should be a 2-dimensional vector, \n",
    "    W should be a 2-dimensional vector,\n",
    "    b should be a scalar.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, W, b):\n",
    "    e = ...\n",
    "    n = ...\n",
    "    for (xi, yi) in zip(X, Y):\n",
    "        # Hint: Use judge() and f_perceptron()\n",
    "        ...\n",
    "    \n",
    "    # Hint: remember we want the average error.\n",
    "    e = ...\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065699e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perceptron_learning(X_train, Y_train):\n",
    "    # Some settings.\n",
    "    errors = []           # Error history.\n",
    "    lam    = 1            # Lambda which controls the step size.\n",
    "\n",
    "    # Initialization.\n",
    "    W      = np.zeros(2)  # Weight.\n",
    "    b      = 0.0          # Bias.\n",
    "    num_iters = 0\n",
    "\n",
    "    # Perceptron learning algorithm.\n",
    "    while calc_error(X_train, Y_train, W, b) > 0 and num_iters < 1000:\n",
    "        for xi,yi in zip(X_train, Y_train):   # Iterate over all data points.\n",
    "            # Compute the model prediction.\n",
    "            yi_pred = ...\n",
    "            # Compare prediction and label.\n",
    "            is_correct = ...\n",
    "            if is_correct:           \n",
    "                continue                      # - If correct, continue.\n",
    "            else:\n",
    "                ...\n",
    "                \n",
    "        # Track training errors. \n",
    "        errors.append(calc_error(X_train, Y_train, W, b))\n",
    "        num_iters += 1\n",
    "    \n",
    "    return W, b, errors\n",
    "\n",
    "W_perceptron, b_perceptron, errors_perceptron = perceptron_learning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abd1ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 5.2\n",
    "Please complete the following codes to visualize the decision boundary of the perceptron model. You may use the `vis` function defined above. \n",
    "\n",
    "Also, please plot the training error curve with respect to the number of iterations.\n",
    "\n",
    "You should only insert your code in the `...` part.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003fe3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show decision boundary, training error and test error.\n",
    "print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(\n",
    "    W_perceptron[0], W_perceptron[1], b_perceptron)\n",
    ")\n",
    "...\n",
    "print('Training error: {}'.format(calc_error(X_train, Y_train, W_perceptron, b_perceptron)))\n",
    "...\n",
    "print('Test error: {}'.format(calc_error(X_test, Y_test, W_perceptron, b_perceptron)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e4b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training error curve.\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca07f612",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 6: Logistic Regression\n",
    "\n",
    "Assume in a binary classification problem, we need to predict a binary label $y \\in \\{−1, 1\\}$ for a feature vector $x = [x_0, x_1]$. In logistic regression, we can reformulate the binary classification problem in a probabilistic framework: We aim to model the distribution of classes given the input feature vector $\\mathbf{x}$. Specifically, we can express the conditional probability $p(y|\\mathbf{x})$ parameterized by $(\\mathbf{w}, b)$ using logistic function as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y|\\mathbf{x})=\\frac{1}{1 + e^{-y(\\mathbf{w} \\cdot \\mathbf{x}+b)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}=[w_0,w_1]^\\top$ is the weight vector, and $b$ is the bias scalar. Given a training dataset $S_\\text{training} = \\{(\\mathbf{x}_i, y_i)\\}, i=1,\\ldots,n\\}$, we wish to optimize the negative log-likelihood loss $\\mathcal{L}(\\mathbf{w},b)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\mathbf{w},b) = -\\sum_{i=1}^n \\ln p(y_i|\\mathbf{x}_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and find the optimal weight vector $\\mathbf{w}$ and bias $b$ to build the logistic regression model:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w}^*, b^* = \\arg \\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this problem, we attempt to obtain the optimal parameters $\\mathbf{w}^*$ and $b^*$ by using a standard gradient descent algorithm. Assume $p_i=p(y_i|\\mathbf{x}_i)$, the gradient for $\\mathbf{w}$ and the gradient for $b$ are shown as following:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial \\mathbf{w}} = -\\sum_{i=1}^n (1 - p_i)y_i \\mathbf{x}_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial b} = -\\sum_{i=1}^n (1 - p_i)y_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In reality, we typically tackle this problem in a matrix form: First, we represent data points as matrices $X = [x_1, x_2, . . . , x_n]^\\top$ and $Y = [y_1, y_2, . . . , y_n]^\\top$. Thus, the negative log-likelihood loss L(w, b) can be formulated as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P = \\text{sigmoid}\\big(Y \\circ (X\\mathbf{w} + b\\mathbf{1}) \\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\mathbf{w},b) = -\\mathbf{1}^\\top \\ln P\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $1 = [1, 1, ..., 1]^\\top \\in \\mathbb{R}^n$ is a n-dimensional column vector, ln(·) is an element-wise natural logarithm function, $\\text{sigmoid}(z) = \\frac{1}{1+e^{-x}}$\n",
    "is an element-wise\n",
    "sigmoid function, and “$\\circ$” is an element-wise product operator. Similarly, we can have the gradient for $\\mathbf{w}$ and $b$ in the matrix form:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial \\mathbf{w}} = -X^\\top\\big((1-P)\\circ Y\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial b} = -\\mathbf{1}^\\top\\big((1-P)\\circ Y\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After obtaining the logistic regression model in Eq. 1 with the optimal $\\mathbf{w}^*$, $b^*$\n",
    "from gradient descent, we can use the following decision rule to predict the label $f(\\mathbf{x}; \\mathbf{w}^*, b^*) \\in \\{−1, 1\\}$ of the feature vector x:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{x}; \\mathbf{w}^*, b^*) = \\begin{cases}\n",
    "1, & \\text{if }p(y=+1|\\mathbf{x})\\geq 0.5, \\\\\n",
    "-1, & \\text{otherwise}. \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Besides, the error $e_{training}$ on the training set $S_{training}=\\{(\\mathbf{x}_i, y_i)\\}$ is defined as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "e_{training} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\big(y_i \\neq f(\\mathbf{x}; \\mathbf{w}^*, b^*)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and we can define the test error $e_{test}$ on the test set $S_{test}$ in the same way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da79353e",
   "metadata": {},
   "source": [
    "#### Load the modified Iris dataset\n",
    "\n",
    "We will use the same dataset as in Question 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = construct_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3084c83",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 6.1 Logistic Regression Using Gradient Descent\n",
    "\n",
    "In this problem, we would like to use the gradient descent to calculate the parameters $\\mathbf{w},b$ for a logistic regression model.\n",
    "If we have the loss function $\\mathcal{L}(\\mathbf{w},b)$, then a typical gradient descent algorithm contains the following steps:\n",
    "\n",
    "**Step 1**. Initialize the parameters $\\mathbf{w}$, $b$.\n",
    "\n",
    "for i = 1 to #iterations:\n",
    "\n",
    "- **Step 2**. Compute the partial derivatives $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}}$, $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$.\n",
    "\n",
    "- **Step 3**. Update the parameters \n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial \\mathbf{w}}, \\quad\\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Note that in the code, we use `W` and `b` to represent the weight vector $\\mathbf{w}$ and bias scalar $b$.\n",
    "\n",
    "**Hint**: For the implementation of some functions, you may refer to HW3 Q4.\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f327831",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Sigmoid function: sigmoid(z) = 1/(1 + e^(-z))\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9776a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Judge function: 1(a != b).\n",
    "def judge(a, b):\n",
    "    \"\"\"\n",
    "    Judge function: 1(a != b).\n",
    "    Return 1 if a != b, otherwise return 0.\n",
    "    \"\"\"\n",
    "    if a != b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def f_logistic(x, W, b):\n",
    "    \"\"\"\n",
    "    Logistic classifier: f(x, W, b)\n",
    "    This function should return -1 or 1.\n",
    "\n",
    "    x should be a 2-dimensional vector, \n",
    "    W should be a 2-dimensional vector,\n",
    "    b should be a scalar.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, W, b):\n",
    "    e = ...\n",
    "    n = ...\n",
    "    for (xi, yi) in zip(X, Y):\n",
    "        # Hint: Use judge() and f_logistic()\n",
    "        ...\n",
    "    \n",
    "    # Hint: remember we want the average error.\n",
    "    e = ...\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c52ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W, b) with respect to W and b.\n",
    "def grad_L_W_b(X, Y, W, b):\n",
    "    ...\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896ac32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss L(W, b).\n",
    "def L_W_b(X, Y, W, b):\n",
    "    ...\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ad644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, Y_train):\n",
    "    # Some settings.\n",
    "    losses = []           # Error history.\n",
    "    learning_rate = 0.001 # Learning rate, fixed\n",
    "    iterations    = 10000 # Iteration number, fixed\n",
    "\n",
    "    # Gradient descent algorithm for logistic regression.\n",
    "    # Step 1. Initialize the parameters W, b.\n",
    "    W      = np.zeros(2)  # Weight.\n",
    "    b      = 0.0          # Bias.\n",
    "\n",
    "    # Logistic regression learning algorithm.\n",
    "    for i in range(iterations):\n",
    "        # Step 2. Compute the partial derivatives.\n",
    "        grad_W, grad_b = ...\n",
    "        # Step 3. Update the parameters.\n",
    "        ...\n",
    "        \n",
    "        # Track the training losses.\n",
    "        ...\n",
    "\n",
    "    return W, b, losses\n",
    "\n",
    "W, b, losses = logistic_regression(X_train, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93c90cd6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 6.2 Visualization\n",
    "Please complete the following codes to visualize the decision boundary of the logistic model in both the training and test data points. \n",
    "\n",
    "You may use the `vis` function defined above. You are supposed to output two plots (one for training, the other for test data).\n",
    "\n",
    "Also, please plot the training error curve with respect to the number of iterations.\n",
    "\n",
    "You should only insert your code in the `...` part.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa56a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show decision boundary, training error and test error.\n",
    "print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(W[0],W[1],b))\n",
    "...\n",
    "print('Training error: {}'.format(calc_error(X_train, Y_train, W, b)))\n",
    "...\n",
    "print('Test error: {}'.format(calc_error(X_test, Y_test, W, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3096ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training error curve.\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5630b26b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# End of A5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33a8d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please make sure to see the output of the gradescope autograder. You are responsible for waiting and ensuring that the autograder is executing normally for your submission. Please create a Piazza post if you see errors in autograder execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a8239",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True, run_tests=True, files=['imgs', 'requirements.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34db08d",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "cogs118a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": false,
   "tests": {
    "Q1_1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1_1\"\npoints = 2\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q1_1_sanity_check(q1_1):\n    all_options = [\"A\", \"B\", \"C\", \"D\"]\n    check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n    assert check_valid(q1_1, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
    "Q1_2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1_2\"\npoints = 2\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q1_2_sanity_check(q1_2):\n    all_options = [\"A\", \"B\", \"C\", \"D\"]\n    check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n    assert check_valid(q1_2, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
    "Q1_3": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1_3\"\npoints = 2\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q1_3_sanity_check(q1_3):\n    all_options = [\"A\", \"B\", \"C\", \"D\"]\n    check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n    assert check_valid(q1_3, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
    "Q3_2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q3_2\"\npoints = 4\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q2_1_sanity_check(env):\n    def has_variable(var):\n        assert var in env, \"Did you define the variable {}?\".format(var)\n    has_variable(\"y1\")\n    has_variable(\"y2\")\n\n    def correct_type(var, expected_type):\n        assert isinstance(env[var], expected_type), \"Did you define {} to be of type {}?\".format(var, expected_type)\n\n    correct_type(\"y1\", int)\n    correct_type(\"y2\", int)\n",
    "Q4_1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q4_1\"\npoints = 4\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q4_sanity_check(env):\n    def has_variable(var):\n        assert var in env, \"Did you define the variable {}?\".format(var)\n    has_variable(\"L_1\")\n    has_variable(\"L_2\")\n    has_variable(\"L_3\")\n    has_variable(\"L_4\")\n\n    def correct_type(var, expected_type):\n        assert isinstance(env[var], expected_type), \"Did you define {} to be of type {}?\".format(var, expected_type)\n\n    correct_type(\"L_1\", int)\n    correct_type(\"L_2\", int)\n    correct_type(\"L_3\", int)\n    correct_type(\"L_4\", int)\n",
    "Q4_2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q4_2\"\npoints = 4\n\n@test_case(points=0, hidden=False)\n# sanity check\ndef q4_sanity_check(env):\n    def has_variable(var):\n        assert var in env, \"Did you define the variable {}?\".format(var)\n    has_variable(\"dL_dw_1\")\n    has_variable(\"dL_dw_2\")\n    has_variable(\"dL_dw_3\")\n    has_variable(\"dL_dw_4\")\n\n    def correct_type(var, expected_type):\n        assert isinstance(env[var], expected_type), \"Did you define {} to be of type {}?\".format(var, expected_type)\n        assert len(env[var]) == 2, \"Did you define {} to be a list of length 2?\".format(var)\n\n    correct_type(\"dL_dw_1\", list)\n    correct_type(\"dL_dw_2\", list)\n    correct_type(\"dL_dw_3\", list)\n    correct_type(\"dL_dw_4\", list)\n",
    "Q5_perceptron_code": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q5_perceptron_code\"\npoints = 8\n\n",
    "Q6_logistic_code": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q6_logistic_code\"\npoints = 8\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
